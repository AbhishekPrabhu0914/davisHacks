{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b302836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of 'kaggleData/dataset1':\n",
      "- .DS_Store\n",
      "- test\n",
      "- train\n",
      "- validation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "face_data_dir = 'kaggleData/dataset1'\n",
    "\n",
    "if os.path.exists(face_data_dir):\n",
    "    print(f\"Contents of '{face_data_dir}':\")\n",
    "    for item in os.listdir(face_data_dir):\n",
    "        item_path = os.path.join(face_data_dir, item)\n",
    "        print(f\"- {item}\")\n",
    "else:\n",
    "    print(f\"Error: Directory '{face_data_dir}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f3e6328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of 'kaggleData/dataset2':\n",
      "- .DS_Store\n",
      "- test\n",
      "- train\n",
      "- validation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "face_data_dir = 'kaggleData/dataset2'\n",
    "\n",
    "if os.path.exists(face_data_dir):\n",
    "    print(f\"Contents of '{face_data_dir}':\")\n",
    "    for item in os.listdir(face_data_dir):\n",
    "        item_path = os.path.join(face_data_dir, item)\n",
    "        print(f\"- {item}\")\n",
    "else:\n",
    "    print(f\"Error: Directory '{face_data_dir}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a07d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling of 'train' complete. Sampled data saved to: kaggleData/dataset1/sampled_train\n",
      "Sampling of 'test' complete. Sampled data saved to: kaggleData/dataset1/sampled_test\n",
      "Sampling of 'validation' complete. Sampled data saved to: kaggleData/dataset1/sampled_val\n",
      "Sampling process completed for train, test, and validation (if present).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "main_dataset_dir = 'kaggleData'\n",
    "dataset_name = 'dataset1'\n",
    "dataset1_path = os.path.join(main_dataset_dir, dataset_name)\n",
    "\n",
    "data_dirs = {'train': 'sampled_train',\n",
    "             'test': 'sampled_test',\n",
    "             'validation': 'sampled_val'}  \n",
    "\n",
    "sampling_ratio = 0.6  # Keep 60% of the original data\n",
    "\n",
    "for original_subdir, sampled_subdir_name in data_dirs.items():\n",
    "    original_dir = os.path.join(dataset1_path, original_subdir)\n",
    "    sampled_dir = os.path.join(dataset1_path, sampled_subdir_name)\n",
    "    os.makedirs(sampled_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(original_dir) and os.path.isdir(original_dir):\n",
    "        for class_name in os.listdir(original_dir):\n",
    "            original_class_dir = os.path.join(original_dir, class_name)\n",
    "            sampled_class_dir = os.path.join(sampled_dir, class_name)\n",
    "            os.makedirs(sampled_class_dir, exist_ok=True)\n",
    "\n",
    "            if os.path.isdir(original_class_dir):\n",
    "                image_files = [f for f in os.listdir(original_class_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                num_to_sample = int(len(image_files) * sampling_ratio)\n",
    "                sampled_files = random.sample(image_files, num_to_sample)\n",
    "\n",
    "                for file_name in sampled_files:\n",
    "                    original_path = os.path.join(original_class_dir, file_name)\n",
    "                    sampled_path = os.path.join(sampled_class_dir, file_name)\n",
    "                    shutil.copy2(original_path, sampled_path)  # Copy with metadata\n",
    "        print(f\"Sampling of '{original_subdir}' complete. Sampled data saved to: {sampled_dir}\")\n",
    "    else:\n",
    "        print(f\"Warning: Directory '{original_dir}' not found.\")\n",
    "\n",
    "print(\"Sampling process completed for train, test, and validation (if present).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f4196f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29809 files belonging to 2 classes.\n",
      "Found 8389 files belonging to 2 classes.\n",
      "Found 3859 files belonging to 3 classes.\n",
      "Class names: ['fake', 'real']\n",
      "Number of classes: 2 \n",
      "\n",
      "Starting training of the tweaked model from scratch...\n",
      "Epoch 1/30\n",
      "932/932 [==============================] - 198s 212ms/step - loss: 0.6005 - accuracy: 0.7177 - val_loss: 0.5067 - val_accuracy: 0.7585\n",
      "Epoch 2/30\n",
      "932/932 [==============================] - 240s 258ms/step - loss: 0.2869 - accuracy: 0.8771 - val_loss: 0.6148 - val_accuracy: 0.7685\n",
      "Epoch 3/30\n",
      "932/932 [==============================] - 237s 254ms/step - loss: 0.1963 - accuracy: 0.9219 - val_loss: 0.3318 - val_accuracy: 0.8529\n",
      "Epoch 4/30\n",
      "932/932 [==============================] - 240s 258ms/step - loss: 0.1556 - accuracy: 0.9377 - val_loss: 0.2242 - val_accuracy: 0.9038\n",
      "Epoch 5/30\n",
      "932/932 [==============================] - 229s 245ms/step - loss: 0.1382 - accuracy: 0.9436 - val_loss: 0.4375 - val_accuracy: 0.8547\n",
      "Epoch 6/30\n",
      "932/932 [==============================] - 228s 245ms/step - loss: 0.1299 - accuracy: 0.9473 - val_loss: 0.2349 - val_accuracy: 0.8943\n",
      "Epoch 7/30\n",
      "932/932 [==============================] - 233s 250ms/step - loss: 0.1115 - accuracy: 0.9552 - val_loss: 0.3903 - val_accuracy: 0.8585\n",
      "Epoch 8/30\n",
      "932/932 [==============================] - 226s 242ms/step - loss: 0.1070 - accuracy: 0.9582 - val_loss: 0.1967 - val_accuracy: 0.9201\n",
      "Epoch 9/30\n",
      "932/932 [==============================] - 216s 232ms/step - loss: 0.0881 - accuracy: 0.9632 - val_loss: 0.2601 - val_accuracy: 0.9143\n",
      "Epoch 10/30\n",
      "932/932 [==============================] - 217s 232ms/step - loss: 0.0873 - accuracy: 0.9640 - val_loss: 0.2158 - val_accuracy: 0.9107\n",
      "Epoch 11/30\n",
      "124/932 [==>...........................] - ETA: 2:58 - loss: 0.0781 - accuracy: 0.9677"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 88\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# --- Train the Model ---\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training of the tweaked model from scratch...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_dataset\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining finished!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# --- Evaluate on the test set ---\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "main_dataset_dir = 'kaggleData'\n",
    "dataset_name = 'dataset1'\n",
    "dataset1_path = os.path.join(main_dataset_dir, dataset_name)\n",
    "image_height = 128\n",
    "image_width = 128\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "learning_rate = 0.0005\n",
    "num_classes = 2\n",
    "\n",
    "sampled_train_dir = os.path.join(dataset1_path, 'sampled_train')\n",
    "sampled_val_dir = os.path.join(dataset1_path, 'sampled_val')\n",
    "sampled_test_dir = os.path.join(dataset1_path, 'sampled_test')\n",
    "\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    sampled_train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    image_size=(image_height, image_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    sampled_val_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    image_size=(image_height, image_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    sampled_test_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24905edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29809 files belonging to 2 classes.\n",
      "Found 8389 files belonging to 2 classes.\n",
      "Found 3859 files belonging to 3 classes.\n",
      "Class names: ['fake', 'real']\n",
      "Number of classes: 2 \n",
      "\n",
      "Starting training with L2 regularization and early stopping...\n",
      "Epoch 1/30\n",
      "932/932 [==============================] - 204s 219ms/step - loss: 0.5851 - accuracy: 0.7477 - val_loss: 0.5811 - val_accuracy: 0.7623\n",
      "Epoch 2/30\n",
      "932/932 [==============================] - 207s 222ms/step - loss: 0.3128 - accuracy: 0.8941 - val_loss: 0.4154 - val_accuracy: 0.8385\n",
      "Epoch 3/30\n",
      "932/932 [==============================] - 220s 236ms/step - loss: 0.2501 - accuracy: 0.9229 - val_loss: 0.3374 - val_accuracy: 0.8927\n",
      "Epoch 4/30\n",
      "932/932 [==============================] - 224s 241ms/step - loss: 0.2212 - accuracy: 0.9370 - val_loss: 0.3326 - val_accuracy: 0.8882\n",
      "Epoch 5/30\n",
      "932/932 [==============================] - 226s 243ms/step - loss: 0.2058 - accuracy: 0.9452 - val_loss: 0.3587 - val_accuracy: 0.8750\n",
      "Epoch 6/30\n",
      "932/932 [==============================] - 224s 241ms/step - loss: 0.1965 - accuracy: 0.9490 - val_loss: 0.6135 - val_accuracy: 0.7833\n",
      "Epoch 7/30\n",
      "932/932 [==============================] - 235s 252ms/step - loss: 0.1903 - accuracy: 0.9533 - val_loss: 0.3784 - val_accuracy: 0.8722\n",
      "Epoch 8/30\n",
      "932/932 [==============================] - 228s 245ms/step - loss: 0.1855 - accuracy: 0.9572 - val_loss: 0.6080 - val_accuracy: 0.8429\n",
      "Epoch 9/30\n",
      "932/932 [==============================] - 226s 243ms/step - loss: 0.1754 - accuracy: 0.9632 - val_loss: 0.3764 - val_accuracy: 0.9056\n",
      "\n",
      "Training finished!\n",
      "\n",
      "Evaluating on the test set...\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'categorical_crossentropy/softmax_cross_entropy_with_logits' defined at (most recent call last):\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/ipykernel/kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/64/_h342z8175d41lnx0r7jdg8r0000gn/T/ipykernel_98772/1566807913.py\", line 104, in <module>\n      loss, accuracy = model.evaluate(test_dataset)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/training.py\", line 2200, in evaluate\n      logs = test_function_runner.run_step(\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/training.py\", line 4000, in run_step\n      tmp_logs = self._function(dataset_or_iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1972, in test_function\n      return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1956, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1944, in run_step\n      outputs = model.test_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1852, in test_step\n      self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n      return self.compiled_loss(\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/losses.py\", line 2122, in categorical_crossentropy\n      return backend.categorical_crossentropy(\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/backend.py\", line 5566, in categorical_crossentropy\n      return tf.nn.softmax_cross_entropy_with_logits(\nNode: 'categorical_crossentropy/softmax_cross_entropy_with_logits'\nlogits and labels must be broadcastable: logits_size=[32,2] labels_size=[32,3]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_test_function_164239]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 104\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# --- Evaluate on the test set ---\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on the test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'categorical_crossentropy/softmax_cross_entropy_with_logits' defined at (most recent call last):\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/ipykernel/kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/sathvikanadipalli/Library/Python/3.8/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/64/_h342z8175d41lnx0r7jdg8r0000gn/T/ipykernel_98772/1566807913.py\", line 104, in <module>\n      loss, accuracy = model.evaluate(test_dataset)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/training.py\", line 2200, in evaluate\n      logs = test_function_runner.run_step(\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/training.py\", line 4000, in run_step\n      tmp_logs = self._function(dataset_or_iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1972, in test_function\n      return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1956, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1944, in run_step\n      outputs = model.test_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1852, in test_step\n      self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n      return self.compiled_loss(\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/losses.py\", line 2122, in categorical_crossentropy\n      return backend.categorical_crossentropy(\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/src/backend.py\", line 5566, in categorical_crossentropy\n      return tf.nn.softmax_cross_entropy_with_logits(\nNode: 'categorical_crossentropy/softmax_cross_entropy_with_logits'\nlogits and labels must be broadcastable: logits_size=[32,2] labels_size=[32,3]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_test_function_164239]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "main_dataset_dir = 'kaggleData'\n",
    "dataset_name = 'dataset1'\n",
    "dataset1_path = os.path.join(main_dataset_dir, dataset_name)\n",
    "image_height = 128\n",
    "image_width = 128\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "learning_rate = 0.0005\n",
    "num_classes = 2\n",
    "l2_lambda = 0.0001\n",
    "\n",
    "sampled_train_dir = os.path.join(dataset1_path, 'sampled_train')\n",
    "sampled_val_dir = os.path.join(dataset1_path, 'sampled_val')\n",
    "sampled_test_dir = os.path.join(dataset1_path, 'sampled_test')\n",
    "\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    sampled_train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    image_size=(image_height, image_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    sampled_val_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    image_size=(image_height, image_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    sampled_test_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    image_size=(image_height, image_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "class_names = train_dataset.class_names\n",
    "print(\"Class names:\", class_names)\n",
    "print(\"Number of classes:\", num_classes, \"\\n\")\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3),\n",
    "                  kernel_regularizer=regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f9cd40",
   "metadata": {},
   "source": [
    "**Pivoting Project Ideas: want to make a chrome extension that allows you to select an image on a screen and determine if it is fake or not**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb2cdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1001 files belonging to 2 classes.\n",
      "Found 388 files belonging to 2 classes.\n",
      "Found 229 files belonging to 2 classes.\n",
      "Sample Training Dataset:\n",
      "Image batch shape: (32, 224, 224, 3)\n",
      "Labels batch shape: (32, 1)\n",
      "\n",
      "Sample Validation Dataset:\n",
      "Image batch shape: (32, 224, 224, 3)\n",
      "Labels batch shape: (32, 1)\n",
      "\n",
      "Sample Test Dataset:\n",
      "Image batch shape: (32, 224, 224, 3)\n",
      "Labels batch shape: (32, 1)\n",
      "\n",
      "Datasets with preprocessing applied:\n",
      "Processed image batch dtype: <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "main_dataset_dir = 'Midjourney_Exp2'\n",
    "\n",
    "image_height = 224\n",
    "image_width = 224\n",
    "batch_size = 32\n",
    "\n",
    "def load_dataset(directory):\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory,\n",
    "        labels='inferred',\n",
    "        label_mode='binary',\n",
    "        image_size=(image_height, image_width),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "train_dir = os.path.join(main_dataset_dir, 'train')\n",
    "valid_dir = os.path.join(main_dataset_dir, 'valid')\n",
    "test_dir = os.path.join(main_dataset_dir, 'test')\n",
    "\n",
    "sample_train = load_dataset(train_dir)\n",
    "sample_valis = load_dataset(valid_dir)\n",
    "sample_test = load_dataset(test_dir)\n",
    "\n",
    "print(\"Sample Training Dataset:\")\n",
    "for image_batch, labels_batch in sample_train.take(1):\n",
    "    print(\"Image batch shape:\", image_batch.numpy().shape)\n",
    "    print(\"Labels batch shape:\", labels_batch.numpy().shape)\n",
    "\n",
    "print(\"\\nSample Validation Dataset:\")\n",
    "for image_batch, labels_batch in sample_valis.take(1):\n",
    "    print(\"Image batch shape:\", image_batch.numpy().shape)\n",
    "    print(\"Labels batch shape:\", labels_batch.numpy().shape)\n",
    "\n",
    "print(\"\\nSample Test Dataset:\")\n",
    "for image_batch, labels_batch in sample_test.take(1):\n",
    "    print(\"Image batch shape:\", image_batch.numpy().shape)\n",
    "    print(\"Labels batch shape:\", labels_batch.numpy().shape)\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    return image, label\n",
    "\n",
    "sample_train = sample_train.map(preprocess)\n",
    "sample_valis = sample_valis.map(preprocess)\n",
    "sample_test = sample_test.map(preprocess)\n",
    "\n",
    "print(\"\\nDatasets with preprocessing applied:\")\n",
    "for image_batch, labels_batch in sample_train.take(1):\n",
    "    print(\"Processed image batch dtype:\", image_batch.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32b5d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allowong SSL verification wasn't working before so pretrained models was not being downloaded\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839644e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " MobilenetV3small (Function  (None, 7, 7, 576)         939120    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d_2  (None, 576)               0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 576)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 577       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 939697 (3.58 MB)\n",
      "Trainable params: 577 (2.25 KB)\n",
      "Non-trainable params: 939120 (3.58 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV3Small  # Or MobileNetV3Large\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "base_model = MobileNetV3Small(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(image_height, image_width, 3)\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "global_average_layer = layers.GlobalAveragePooling2D()\n",
    "prediction_layer = layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    global_average_layer,\n",
    "    layers.Dropout(0.5),\n",
    "    prediction_layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1dfd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 11s 302ms/step - loss: 0.7051 - accuracy: 0.6254 - val_loss: 0.5786 - val_accuracy: 0.7062\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 11s 298ms/step - loss: 0.6480 - accuracy: 0.6474 - val_loss: 0.5549 - val_accuracy: 0.7216\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 11s 299ms/step - loss: 0.6145 - accuracy: 0.6893 - val_loss: 0.5348 - val_accuracy: 0.7577\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 11s 295ms/step - loss: 0.5658 - accuracy: 0.7213 - val_loss: 0.5212 - val_accuracy: 0.7680\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 11s 299ms/step - loss: 0.5465 - accuracy: 0.7313 - val_loss: 0.5164 - val_accuracy: 0.7706\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 11s 295ms/step - loss: 0.5308 - accuracy: 0.7443 - val_loss: 0.5041 - val_accuracy: 0.7784\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 11s 304ms/step - loss: 0.4917 - accuracy: 0.7682 - val_loss: 0.4977 - val_accuracy: 0.7835\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 11s 301ms/step - loss: 0.4672 - accuracy: 0.7852 - val_loss: 0.4904 - val_accuracy: 0.7887\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 11s 302ms/step - loss: 0.4652 - accuracy: 0.7782 - val_loss: 0.4894 - val_accuracy: 0.7964\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 11s 304ms/step - loss: 0.4582 - accuracy: 0.7862 - val_loss: 0.4812 - val_accuracy: 0.8041\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m      4\u001b[0m     sample_train,\n\u001b[1;32m      5\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m      6\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39msample_valis\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Evaluate the trained model on the test dataset\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(\u001b[43mtest_dataset\u001b[49m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 10  \n",
    "\n",
    "history = model.fit(\n",
    "    sample_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=sample_valis\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ed9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 61ms/step - loss: 0.3549 - accuracy: 0.8777\n",
      "Test Loss: 0.3549\n",
      "Test Accuracy: 0.8777\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(sample_test) \n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e4b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for Individual Test Images:\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Image: 0012.jpg, Predicted Probability (Fake): 0.0925\n",
      "Prediction: Likely REAL\n",
      "------------------------------\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Image: Screenshot 2025-04-19 at 7.11.30 PM.jpg, Predicted Probability (Fake): 0.8813\n",
      "Prediction: Likely FAKE\n",
      "------------------------------\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Image: 0006.jpg, Predicted Probability (Fake): 0.2969\n",
      "Prediction: Likely REAL\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "test_image_paths = ['0012.jpg',\n",
    "                    'Screenshot 2025-04-19 at 7.11.30 PM.jpg',\n",
    "                    '0006.jpg']\n",
    "\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(img_height, img_width))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = tf.image.convert_image_dtype(img_array, dtype=tf.float32)\n",
    "    return img_array\n",
    "\n",
    "print(\"Predictions for Individual Test Images:\")\n",
    "for img_path in test_image_paths:\n",
    "    try:\n",
    "        processed_img = preprocess_image(img_path)\n",
    "        prediction = model.predict(processed_img)[0][0]\n",
    "        print(f\"Image: {img_path}, Predicted Probability (Fake): {prediction:.4f}\")\n",
    "        if prediction > 0.5:\n",
    "            print(\"Prediction: Likely FAKE\")\n",
    "        else:\n",
    "            print(\"Prediction: Likely REAL\")\n",
    "        print(\"-\" * 30)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Image not found at {img_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {img_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab4ed39",
   "metadata": {},
   "source": [
    "**Start of Our Large Dataset Training and Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb6544a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39404 files belonging to 2 classes.\n",
      "Found 9863 files belonging to 2 classes.\n",
      "Your Training Dataset:\n",
      "Image batch shape: (32, 224, 224, 3)\n",
      "Labels batch shape: (32, 1)\n",
      "\n",
      "Your Validation Dataset:\n",
      "Image batch shape: (32, 224, 224, 3)\n",
      "Labels batch shape: (32, 1)\n",
      "\n",
      "Your Test Dataset:\n",
      "Image batch shape: (32, 224, 224, 3)\n",
      "Labels batch shape: (32, 1)\n",
      "\n",
      "Datasets with preprocessing applied:\n",
      "Processed image batch dtype: <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "main_dataset_dir = 'newarchive' \n",
    "image_height = 224\n",
    "image_width = 224\n",
    "batch_size = 32\n",
    "\n",
    "train_dir = os.path.join(main_dataset_dir, 'train')\n",
    "test_dir = os.path.join(main_dataset_dir, 'test')\n",
    "\n",
    "def load_dataset(directory):\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory,\n",
    "        labels='inferred',  \n",
    "        label_mode='binary', \n",
    "        image_size=(image_height, image_width),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "sample_train = load_dataset(train_dir)\n",
    "sample_test = load_dataset(test_dir)\n",
    "\n",
    "#make validation ste from training set\n",
    "cardinality = tf.data.experimental.cardinality(sample_train)\n",
    "val_size_float = tf.cast(cardinality, tf.float32) * 0.2\n",
    "val_size = tf.cast(tf.round(val_size_float), tf.int64)\n",
    "sample_valid = sample_train.take(val_size)\n",
    "sample_train = sample_train.skip(val_size)\n",
    "\n",
    "print(\"Your Training Dataset:\")\n",
    "for image_batch, labels_batch in sample_train.take(1):\n",
    "    print(\"Image batch shape:\", image_batch.numpy().shape)\n",
    "    print(\"Labels batch shape:\", labels_batch.numpy().shape)\n",
    "\n",
    "print(\"\\nYour Validation Dataset:\")\n",
    "for image_batch, labels_batch in sample_valid.take(1):\n",
    "    print(\"Image batch shape:\", image_batch.numpy().shape)\n",
    "    print(\"Labels batch shape:\", labels_batch.numpy().shape)\n",
    "\n",
    "print(\"\\nYour Test Dataset:\")\n",
    "for image_batch, labels_batch in sample_test.take(1):\n",
    "    print(\"Image batch shape:\", image_batch.numpy().shape)\n",
    "    print(\"Labels batch shape:\", labels_batch.numpy().shape)\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    return image, label\n",
    "\n",
    "sample_train = sample_train.map(preprocess)\n",
    "sample_valid = sample_valid.map(preprocess)\n",
    "sample_test = sample_test.map(preprocess)\n",
    "\n",
    "print(\"\\nDatasets with preprocessing applied:\")\n",
    "for image_batch, labels_batch in sample_train.take(1):\n",
    "    print(\"Processed image batch dtype:\", image_batch.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3992c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " MobilenetV3large (Function  (None, 7, 7, 960)         2996352   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 960)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 960)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 961       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2997313 (11.43 MB)\n",
      "Trainable params: 961 (3.75 KB)\n",
      "Non-trainable params: 2996352 (11.43 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "image_height = 224\n",
    "image_width = 224\n",
    "\n",
    "large_base_model = MobileNetV3Large(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(image_height, image_width, 3)\n",
    ")\n",
    "\n",
    "# Freeze the base model layers\n",
    "large_base_model.trainable = False\n",
    "\n",
    "large_global_average_layer = layers.GlobalAveragePooling2D()\n",
    "large_prediction_layer = layers.Dense(1, activation='sigmoid') \n",
    "\n",
    "largemodel = models.Sequential([\n",
    "    large_base_model,\n",
    "    large_global_average_layer,\n",
    "    layers.Dropout(0.5),\n",
    "    large_prediction_layer\n",
    "])\n",
    "\n",
    "largemodel.compile(optimizer='adam',\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "largemodel.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df1ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "986/986 [==============================] - 572s 492ms/step - loss: 0.4741 - accuracy: 0.7840 - val_loss: 0.3324 - val_accuracy: 0.8604\n",
      "Epoch 2/5\n",
      "986/986 [==============================] - 589s 509ms/step - loss: 0.4011 - accuracy: 0.8245 - val_loss: 0.3251 - val_accuracy: 0.8671\n",
      "Epoch 3/5\n",
      "986/986 [==============================] - 607s 527ms/step - loss: 0.3997 - accuracy: 0.8244 - val_loss: 0.3227 - val_accuracy: 0.8725\n",
      "Epoch 4/5\n",
      "986/986 [==============================] - 603s 523ms/step - loss: 0.3942 - accuracy: 0.8269 - val_loss: 0.3207 - val_accuracy: 0.8689\n",
      "Epoch 5/5\n",
      "986/986 [==============================] - 608s 528ms/step - loss: 0.3950 - accuracy: 0.8266 - val_loss: 0.3308 - val_accuracy: 0.8579\n",
      "309/309 [==============================] - 133s 421ms/step - loss: 0.3314 - accuracy: 0.8590\n",
      "Test Loss (Large Model): 0.3314\n",
      "Test Accuracy (Large Model): 0.8590\n"
     ]
    }
   ],
   "source": [
    "epochs = 5 \n",
    "\n",
    "history = largemodel.fit(\n",
    "    sample_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=sample_valid\n",
    ")\n",
    "\n",
    "loss, accuracy = largemodel.evaluate(sample_test)\n",
    "print(f\"Test Loss (Large Model): {loss:.4f}\")\n",
    "print(f\"Test Accuracy (Large Model): {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6353f82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309/309 [==============================] - 123s 387ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.40      0.32      0.35      3875\n",
      "        Real       0.61      0.68      0.64      5988\n",
      "\n",
      "    accuracy                           0.54      9863\n",
      "   macro avg       0.50      0.50      0.50      9863\n",
      "weighted avg       0.52      0.54      0.53      9863\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "predictions = largemodel.predict(sample_test)\n",
    "predicted_classes = np.round(predictions).flatten() \n",
    "\n",
    "\n",
    "true_labels = np.concatenate([y.numpy() for _, y in sample_test])\n",
    "\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_classes)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predicted_classes, target_names=['Fake', 'Real']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
